from datetime import datetime
from typing import List

from srdt_analysis.albert import AlbertBase
from srdt_analysis.chunk import Chunker
from srdt_analysis.collections import Collections
from srdt_analysis.llm import LLMProcessor
from srdt_analysis.models import DocumentData, DocumentsList
from srdt_analysis.save import DocumentProcessor
from srdt_analysis.vector import Vector


class BaseDataExploiter:
    def __init__(self):
        self.llm_processor = LLMProcessor()
        self.vector_processor = Vector()
        self.doc_processor = DocumentProcessor()
        self.chunker = Chunker()
        self.collections = Collections()
        self.albert = AlbertBase()

    def process_documents(
        self, data: DocumentsList, output_file: str, collection_name: str
    ):
        results: List[DocumentData] = []
        print(f"Number of articles to be processed: {len(data)}")

        for doc in data:
            print(
                f"[{datetime.now().strftime('%H:%M:%S')}] Processing article: {doc.title}"
            )
            content = self.get_content(doc)

            chunks = self.chunker.split(content)

            summary = self.llm_processor.get_summary(content)
            keywords = self.llm_processor.get_keywords(content)
            questions = self.llm_processor.get_questions(content)


            doc_data = self.create_document_data(
                doc, content, chunks, keywords, summary, questions
            )
            results.append(doc_data)

            print(
                f"[{datetime.now().strftime('%H:%M:%S')}] Article number {data.index(doc) + 1} out of {len(data)} processed"
            )

        self.doc_processor.save_to_csv(results, output_file)
        id = self.collections.create(collection_name)
        self.collections.upload(results, id)
        return (results, id)

    def create_document_data(
        self, doc, content, chunks, keywords, summary, questions
    ) -> DocumentData:
        base_data = {
            "cdtn_id": doc.cdtn_id,
            "initial_id": doc.initial_id,
            "title": doc.title,
            "content": content,
            "keywords": keywords,
            "summary": summary,
            "questions": questions,
            "chunks": chunks,
            # "vector_chunks": [self.vector_processor.generate(chunk) for chunk in chunks],
            # "vector_questions": self.vector_processor.generate(questions),
            # "vector_summary": self.vector_processor.generate(summary),
            # "vector_keywords": self.vector_processor.generate(keywords),
        }
        return self.doc_processor.process_document(**base_data)


class ArticlesCodeDuTravailExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return doc.text


class FichesMTExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return "".join(
            section.get("html", "") for section in doc.document.get("sections", [])
        )


class FichesSPExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return doc.document.get("raw", "")


class PageInfosExploiter(BaseDataExploiter):
    def get_content(self, doc):
        markdown = ""
        for content in doc.document.get("contents", []):
            for block in content.get("blocks", []):
                if block.get("type") == "markdown":
                    markdown += block.get("markdown", "")
        return markdown


class PageContribsExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return doc.document.get("content", "")

    def create_document_data(self, doc, content, chunks, keywords, summary, questions):
        data = super().create_document_data(
            doc, content, chunks, keywords, summary, questions
        )
        data["idcc"] = doc.document.get("idcc", "0000")
        return data
