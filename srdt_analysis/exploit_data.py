from srdt_analysis.data import get_data
from srdt_analysis.models import DocumentsList
from srdt_analysis.llm import LLMProcessor
from srdt_analysis.save import DocumentProcessor
from srdt_analysis.vector import Vector
from datetime import datetime


def exploit_data():
    result = get_data()
    # exploit_articles_code_du_travail(result[0])
    exploit_articles_code_du_travail([result[0][0]])
    # exploit_fiches_mt(result[1])
    exploit_fiches_mt([result[1][0]])
    # exploit_fiches_sp(result[2])
    exploit_fiches_sp([result[2][0]])
    # exploit_page_infos(result[3])
    exploit_page_infos([result[3][0]])
    # exploit_page_contribs(result[4])
    exploit_page_contribs([result[4][0]])


def exploit_articles_code_du_travail(data: DocumentsList):
    results = []
    llm_processor = LLMProcessor()
    vector_processor = Vector()
    doc_processor = DocumentProcessor()

    print(f"Number of articles to be processed: {len(data)}")
    for doc in data:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Processing article: {doc.title}")
        summary = llm_processor.get_summary(doc.text)
        keywords = llm_processor.get_keywords(doc.text)
        doc_data = doc_processor.process_document(
            cdtn_id=doc.cdtn_id,
            initial_id=doc.initial_id,
            title=doc.title,
            content=doc.text,
            keywords=keywords,
            summary=summary,
            vector_summary=vector_processor.generate(summary),
            vector_keywords=vector_processor.generate(keywords),
        )
        print(
            f"[{datetime.now().strftime('%H:%M:%S')}] Article number {data.index(doc) + 1} out of {len(data)} processed"
        )
        results.append(doc_data)
    doc_processor.save_to_csv(results, "articles_code_du_travail.csv")
    return results


def exploit_fiches_mt(data: DocumentsList):
    results = []
    llm_processor = LLMProcessor()
    vector_processor = Vector()
    doc_processor = DocumentProcessor()

    print(f"Number of articles to be processed: {len(data)}")
    for doc in data:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Processing article: {doc.title}")
        concatenated_html = ""
        sections = doc.document.get("sections", [])
        for section in sections:
            concatenated_html += section.get("html", "")
        summary = llm_processor.get_summary(concatenated_html)
        keywords = llm_processor.get_keywords(concatenated_html)
        doc_data = doc_processor.process_document(
            cdtn_id=doc.cdtn_id,
            initial_id=doc.initial_id,
            title=doc.title,
            content=concatenated_html,
            keywords=keywords,
            summary=summary,
            vector_summary=vector_processor.generate(summary),
            vector_keywords=vector_processor.generate(keywords),
        )
        print(
            f"[{datetime.now().strftime('%H:%M:%S')}] Article number {data.index(doc) + 1} out of {len(data)} processed"
        )
        results.append(doc_data)
    doc_processor.save_to_csv(results, "fiches_mt.csv")
    return results


def exploit_fiches_sp(data: DocumentsList):
    results = []
    llm_processor = LLMProcessor()
    vector_processor = Vector()
    doc_processor = DocumentProcessor()

    print(f"Number of articles to be processed: {len(data)}")
    for doc in data:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Processing article: {doc.title}")
        content = doc.document.get("raw", "")
        summary = llm_processor.get_summary(content)
        keywords = llm_processor.get_keywords(content)
        doc_data = doc_processor.process_document(
            cdtn_id=doc.cdtn_id,
            initial_id=doc.initial_id,
            title=doc.title,
            content=content,
            keywords=keywords,
            summary=summary,
            vector_summary=vector_processor.generate(summary),
            vector_keywords=vector_processor.generate(keywords),
        )
        print(
            f"[{datetime.now().strftime('%H:%M:%S')}] Article number {data.index(doc) + 1} out of {len(data)} processed"
        )
        results.append(doc_data)
    doc_processor.save_to_csv(results, "fiches_sp.csv")
    return results


def exploit_page_infos(data: DocumentsList):
    results = []
    llm_processor = LLMProcessor()
    vector_processor = Vector()
    doc_processor = DocumentProcessor()

    print(f"Number of articles to be processed: {len(data)}")
    for doc in data:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Processing article: {doc.title}")
        concatenated_markdown = ""
        contents = doc.document.get("contents", [])
        for content in contents:
            blocks = content.get("blocks", [])
            for block in blocks:
                if block.get("type") == "markdown":
                    concatenated_markdown += block.get("markdown", "")
        summary = llm_processor.get_summary(concatenated_markdown)
        keywords = llm_processor.get_keywords(concatenated_markdown)
        doc_data = doc_processor.process_document(
            cdtn_id=doc.cdtn_id,
            initial_id=doc.initial_id,
            title=doc.title,
            content=concatenated_markdown,
            keywords=keywords,
            summary=summary,
            vector_summary=vector_processor.generate(summary),
            vector_keywords=vector_processor.generate(keywords),
        )
        print(
            f"[{datetime.now().strftime('%H:%M:%S')}] Article number {data.index(doc) + 1} out of {len(data)} processed"
        )
        results.append(doc_data)
    doc_processor.save_to_csv(results, "page_infos.csv")
    return results


def exploit_page_contribs(data: DocumentsList):
    results = []
    llm_processor = LLMProcessor()
    vector_processor = Vector()
    doc_processor = DocumentProcessor()

    print(f"Number of articles to be processed: {len(data)}")
    for doc in data:
        print(f"[{datetime.now().strftime('%H:%M:%S')}] Processing article: {doc.title}")
        content = doc.document.get("content", "")
        summary = llm_processor.get_summary(content)
        keywords = llm_processor.get_keywords(content)
        doc_data = doc_processor.process_document(
            cdtn_id=doc.cdtn_id,
            initial_id=doc.initial_id,
            title=doc.title,
            content=content,
            keywords=keywords,
            summary=summary,
            vector_summary=vector_processor.generate(summary),
            vector_keywords=vector_processor.generate(keywords),
            idcc=doc.document.get("idcc", "0000"),
        )
        print(
            f"[{datetime.now().strftime('%H:%M:%S')}] Article number {data.index(doc) + 1} out of {len(data)} processed"
        )
        results.append(doc_data)
    doc_processor.save_to_csv(results, "page_contribs.csv")
    return results
