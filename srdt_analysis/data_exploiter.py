from datetime import datetime

from srdt_analysis.albert import AlbertBase
from srdt_analysis.chunker import Chunker
from srdt_analysis.collections import Collections
from srdt_analysis.constants import BASE_URL_CDTN, MODEL_VECTORISATION
from srdt_analysis.document_processor import DocumentProcessor
from srdt_analysis.llm_processor import LLMProcessor
from srdt_analysis.models import DocumentData, DocumentsList, ResultProcessDocumentType


class BaseDataExploiter:
    def __init__(self):
        self.llm_processor = LLMProcessor()
        self.doc_processor = DocumentProcessor()
        self.chunker = Chunker()
        self.collections = Collections()
        self.albert = AlbertBase()

    def process_documents(
        self, data: DocumentsList, output_file: str, collection_name: str
    ) -> ResultProcessDocumentType:
        results: list[DocumentData] = []
        print(f"Number of articles to be processed: {len(data)}")

        for doc in data:
            print(
                f"[{datetime.now().strftime('%H:%M:%S')}] Processing article: {doc.title}"
            )
            content = self.get_content(doc)

            chunks = self.chunker.split(content)

            summary = self.llm_processor.get_summary(content)
            keywords = self.llm_processor.get_keywords(content)
            questions = self.llm_processor.get_questions(content)

            doc_data = self.create_document_data(
                doc, content, chunks, keywords, summary, questions
            )
            results.append(doc_data)

            print(
                f"[{datetime.now().strftime('%H:%M:%S')}] Article number {data.index(doc) + 1} out of {len(data)} processed"
            )

        self.doc_processor.save_to_csv(results, output_file)
        id = self.collections.create(collection_name, MODEL_VECTORISATION)
        self.collections.upload(results, id)
        return {"documents": results, "id": id}

    def create_document_data(
        self, doc, content, chunks, keywords, summary, questions
    ) -> DocumentData:
        return {
            "cdtn_id": doc.cdtn_id,
            "initial_id": doc.initial_id,
            "title": doc.title,
            "content": content,
            "keywords": keywords,
            "summary": summary,
            "questions": questions,
            "content_chunked": chunks,
            "url": BASE_URL_CDTN + "/" + doc.source + "/" + doc.slug,
        }


class ArticlesCodeDuTravailExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return doc.text


class FichesMTExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return "".join(
            section.get("html", "") for section in doc.document.get("sections", [])
        )


class FichesSPExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return doc.document.get("raw", "")


class PageInfosExploiter(BaseDataExploiter):
    def get_content(self, doc):
        markdown = ""
        for content in doc.document.get("contents", []):
            for block in content.get("blocks", []):
                if block.get("type") == "markdown":
                    markdown += block.get("markdown", "")
        return markdown


class PageContribsExploiter(BaseDataExploiter):
    def get_content(self, doc):
        return doc.document.get("content", "")

    def create_document_data(self, doc, content, chunks, keywords, summary, questions):
        data = super().create_document_data(
            doc, content, chunks, keywords, summary, questions
        )
        data["idcc"] = doc.document.get("idcc", "0000")
        return data
